{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2c81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KIIT\\Downloads\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Extracting and processing dataset...\n",
      "\n",
      "Class distribution:\n",
      " - Bonafide (0): 28539\n",
      " - Spoof (1): 0\n",
      "\n",
      "Creating datasets...\n"
     ]
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import tempfile\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import audiomentations as am\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration setup\n",
    "config = {\n",
    "    'sample_rate': 16000,\n",
    "    'duration': 3,\n",
    "    'hop_length': 512,\n",
    "    'n_mels': 128,\n",
    "    'n_fft': 1024,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'patience': 8,\n",
    "    'learning_rate': 0.0005,\n",
    "    'augment_prob': 0.7  # Probability of applying augmentation\n",
    "}\n",
    "\n",
    "#Data augmentation setup\n",
    "augmenter = am.Compose([\n",
    "    am.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.3),\n",
    "    am.PitchShift(min_semitones=-4, max_semitones=4, p=0.3),\n",
    "    am.TimeStretch(min_rate=0.8, max_rate=1.2, p=0.3),\n",
    "    am.Gain(min_gain_db=-6, max_gain_db=6, p=0.2),\n",
    "])\n",
    "\n",
    "def extract_dataset(compressed_path):\n",
    "  #facing problem with extraction\n",
    "    extracted_dir = tempfile.mkdtemp()\n",
    "    with tarfile.open(compressed_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extracted_dir)\n",
    "    \n",
    "    #Discover all audio files\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(extracted_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.flac') or file.endswith('.wav'):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    \n",
    "    #Create labels- problem\n",
    "    labels = []\n",
    "    for file in audio_files:\n",
    "        if 'spoof' in file.lower() or 'fake' in file.lower():\n",
    "            labels.append(1)  # Spoof\n",
    "        else:\n",
    "            labels.append(0)  # Bonafide\n",
    "    \n",
    "    return audio_files, labels, extracted_dir\n",
    "\n",
    "def apply_augmentation(audio, sr):\n",
    "    if np.random.rand() < config['augment_prob']:\n",
    "        return augmenter(samples=audio, sample_rate=sr)\n",
    "    return audio\n",
    "\n",
    "def load_and_preprocess(file_path, label, augment=False):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=config['sample_rate'])\n",
    "        \n",
    "        #Ensure consistent length\n",
    "        if len(audio) > config['sample_rate'] * config['duration']:\n",
    "            audio = audio[:config['sample_rate'] * config['duration']]\n",
    "        else:\n",
    "            audio = np.pad(audio, (0, max(0, config['sample_rate'] * config['duration'] - len(audio))))\n",
    "        \n",
    "        #Apply augmentation only to training data\n",
    "        if augment:\n",
    "            audio = apply_augmentation(audio, sr)\n",
    "        \n",
    "        #Extract mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=config['sample_rate'],\n",
    "            n_fft=config['n_fft'],\n",
    "            hop_length=config['hop_length'],\n",
    "            n_mels=config['n_mels']\n",
    "        )\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min() + 1e-8)\n",
    "        return np.expand_dims(mel_spec, axis=-1), label\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def create_dataset(file_paths, labels, augment=False): #using tensorflow\n",
    "    features = []\n",
    "    processed_labels = []\n",
    "    \n",
    "    for path, label in zip(file_paths, labels):\n",
    "        feat, lbl = load_and_preprocess(path, label, augment=augment)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "            processed_labels.append(lbl)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = to_categorical(processed_labels, num_classes=2)\n",
    "    \n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels)) \\\n",
    "                         .shuffle(len(features)) \\\n",
    "                         .batch(config['batch_size']) \\\n",
    "                         .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "def build_model(input_shape): #Combining CRNN with attention\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    #Enhanced CNN Block\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    #RNN Block with Attention\n",
    "    x = layers.Reshape((-1, x.shape[2]*x.shape[3]))(x)\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    #Multi-head Attention\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    #Classifier\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for x, y in dataset:\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(model.predict(x))\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        np.argmax(y_true, axis=1),\n",
    "        np.argmax(y_pred, axis=1),\n",
    "        target_names=['bonafide', 'spoof']\n",
    "    ))\n",
    "    \n",
    "    #Confusion Matrix\n",
    "    cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['bonafide', 'spoof'],\n",
    "                yticklabels=['bonafide', 'spoof'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    dataset_path = r\"C:\\Users\\KIIT\\AppData\\Local\\Temp\\train-clean-100.tar.gz\"\n",
    "    \n",
    "    #Extract and get file paths with labels\n",
    "    print(\"Extracting and processing dataset...\")\n",
    "    audio_files, labels, _ = extract_dataset(dataset_path)\n",
    "    \n",
    "    #Check class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(f\" - Bonafide (0): {Counter(labels)[0]}\")\n",
    "    print(f\" - Spoof (1): {Counter(labels)[1]}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        audio_files, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    #Create datasets\n",
    "    print(\"\\nCreating datasets...\")\n",
    "    train_ds = create_dataset(train_paths, train_labels, augment=True)\n",
    "    val_ds = create_dataset(test_paths, test_labels, augment=False)\n",
    "    \n",
    "    sample, _ = load_and_preprocess(train_paths[0], train_labels[0])\n",
    "    model = build_model(sample.shape)\n",
    "    model.compile(\n",
    "        optimizer=Adam(config['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    #Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=config['patience'], restore_best_weights=True),\n",
    "        ModelCheckpoint('best_anti_spoof_model.h5', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    #Training\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    #Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Evaluate\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    evaluate_model(model, val_ds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import audiomentations\n",
    "    except ImportError:\n",
    "        print(\"Installing required packages...\")\n",
    "        import subprocess\n",
    "        subprocess.run(['pip', 'install', 'audiomentations'])\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43a1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (env)",
   "language": "python",
   "name": "py35_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
